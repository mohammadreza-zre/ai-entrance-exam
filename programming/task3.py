# -*- coding: utf-8 -*-
"""task3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sQN3oPBo2K9RmHFsQ568r1HHZTTM__aj

** hazm package ---> برای اصلاح نیم فاصله ها و شمارش لغات در زبان فارسی
"""

!pip install hazm
from google.colab import files
import nltk
from nltk import tokenize
nltk.download('punkt')
import re
from hazm import *
import hazm

files.upload()

text = list()
path = "/content/text.txt"
f = open(path)
for line in f.readlines() :
  text.append(line)

print(text)
print(type(text))
print(len(text))

"""    replacing arabic charachters with perian alternatives"""

processed_text = list()
for sent in text :
    sent = re.sub("ك" ,
                  "ک",
                  sent)


    sent = re.sub("،ي" ,
                  "ی",
                  sent)

    sent = re.sub("ة" ,
                  "ه" ,
                  sent)
    processed_text.append(sent)


for t in processed_text:
  print(t)

"""    input : perian number in string format
    output : english number in integer format
"""

def PerisanStringToEnglishInt(number):
  singleNumberToEbglish = {  
      "۱" :"1" ,
      "۲" :"2" ,
      "۳" :"3" ,
      "۴" :"4" ,
      "۵" :"5" ,
      "۶" :"6" ,
      "۷" :"7" ,
      "۸" :"8" ,
      "۹" :"9" ,
      "۰" :"0" ,
      }
  numToE = ""
  for ch in number :
    numToE += singleNumberToEbglish[ch]
  return int(numToE)

"""    Main recursive function

    input : english number in integer format
    output : persian text

    procedure : dividing input number sequentially untill its value become less than 20
"""

def int_to_en(num):
    d = { 0 : 'صفر',
         1 : 'یک',
         2 : 'دو',
         3 : 'سه',
         4 : 'چهار', 
         5 : 'پنج',
          6 : 'شش',
          7 : 'هفت',
          8 : 'هشت',
          9 : 'نه',
          10 : 'ده',
          11 : 'یازده',
          12 : 'دوارده',
          13 : 'سیزده',
          14 : 'چهارده',
          15 : 'پانزده',
          16 : 'شانزده',
          17 : 'هفده',
          18 : 'هجده',
          19 : 'نوزده',
          20 : 'بیست',
          30 : 'سی',
          40 : 'چهل',
          50 : 'پنجاه',
          60 : 'شصت',
          70 : 'هفتاد',
          80 : 'هشتاد',
          90 : 'نود' , 
          100 : 'صد' ,
          200 : 'دویست' , 
          300 : 'سیصد' , 
          400 : 'چهارصد' , 
          500 : 'پانصد' , 
          600 : 'ششصد' , 
          700 : 'هفتصد' , 
          800 : 'هشتصد' , 
          900 : 'نهصد' ,
          }
    k = 1000
    m = 1000000
    b = 1000000000
    t = 1000000000000

    assert(0 <= num)
    if (num < 20):
        return d[num]


    if (num < 100):
        if num % 10 == 0: 
          return d[num]
        else: 
          return d[num // 10 * 10] + ' و '  + d[num % 10]

    if (num < k):
        if num % 100 == 0: 
          return d[num] 
        else:
           return d[(num // 100)*100] + ' و ' + int_to_en(num % 100)

    if (num < m):
        if (num % k) == 0:
           return int_to_en(num // k) + ' هزار'
        else:
           return int_to_en(num // k) + ' هزار و ' + int_to_en(num % k)

    if (num < b):
        if (num % m) == 0: 
          return int_to_en(num // m) + ' میلیون'
        else: 
          return int_to_en(num // m) + ' میلیون و ' + int_to_en(num % m)

    if (num < t):
        if (num % b) == 0:
           return int_to_en(num // b) + ' میلیارد'
        else: 
          return int_to_en(num // b) + ' میلیارد و ' + int_to_en(num % b)

    if (num % t == 0): 
      return int_to_en(num // t) + ' تریلیون'
    else:
       return int_to_en(num // t) + ' تریلیون و ' + int_to_en(num % t)

    raise AssertionError('num is too large: %s' % str(num))

"""    loop over each sentence ---> loop over each word of sentence ---> eliminate excessive charachters (+ applying above functions)  """

alphabets = "۱۲۳۴۵۶۷۸۹۰ضصثقفغعهخحجچشسیبلاتنمکگظطزرذدپو"
alphabets_list = [char for char in alphabets]

processed_sentences = list()
for sent in processed_text :
  processed_words = list()

  words = tokenize.word_tokenize(sent) 
  for word in words : 
    clearWord = re.sub("[^ض ص ث ق ف غ ع ه خ ح ج چ ش س ی ب ل ا ت ن م ک گ ظ ط ز ر ذ د پ و ]" , "", word)
    processed_words.append(clearWord)

    num = re.findall("[۱ ۲ ۳ ۴ ۵ ۶ ۷ ۸ ۹ ۰]" , word) 
    if num :
      numString = ""
      for digit in num :
        numString += digit
      myNum = PerisanStringToEnglishInt(numString)
      myNumToText = int_to_en(myNum)
      processed_words.append(myNumToText)
  processed_sentences.append(processed_words)





finalSent = list()
normalizer = Normalizer()



for ps in processed_sentences : 
  sent = ""    
  for word in ps :
    sent += word + " "
  sentence = normalizer.normalize(sent)
  finalSent.append(sentence)

for i in (finalSent) :
  print(i)

"""    Just keep sentence if its length is less tha 50 or more than 10 """

filerdFinalSenteces = list()
for s in finalSent :
  sWordsList = hazm.word_tokenize(s)
  length = len(sWordsList)
  if  50 >= length >= 10:
    filerdFinalSenteces.append(s)
    print(s)
    print('-----------------')